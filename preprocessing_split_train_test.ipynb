{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split datasets into train, validation, and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module can use for processing split datasets. You need modify the ratio of train, validation, and test. And you can modify output directory you want and input directory you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\" Split datasets into train, validation, and test\n",
    "\n",
    "This module can use for processing split datasets. You need modify the ratio of \n",
    "train, validation, and test datasetes. And you can modify output directory you \n",
    "want and  input directory you have.\n",
    "\n",
    "################################################################################\n",
    "# Author: Weikun Han <weikunhan@gmail.com>\n",
    "# Crate Date: 03/6/2018        \n",
    "# Update:\n",
    "# Reference: https://github.com/jhetherly/EnglishSpeechUpsampler\n",
    "################################################################################\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def write_csv(filename, pairs):\n",
    "    \"\"\"The function to wirte\n",
    "\n",
    "    Args:\n",
    "        param1 (str): filename \n",
    "        param2 (list): pairs\n",
    "\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        for n in pairs:\n",
    "            writer.writerow(n)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Please modify input path  to locate you file\n",
    "    DATASETS_ROOT_DIR = './datasets'\n",
    "    OUTPUT_DIR = os.path.join(DATASETS_ROOT_DIR, 'final_dataset')\n",
    "\n",
    "    # Define ratio for train, validation, and test datasetes\n",
    "    train_fraction = 0.6\n",
    "    validation_fraction = 0.2\n",
    "    test_fraction = 0.2\n",
    "\n",
    "    # Reset random generator\n",
    "    np.random.seed(0)    \n",
    "    \n",
    "    # Check location to save datasets\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        \n",
    "    original_noise_pairs = []\n",
    "    input_original_path = os.path.join(DATASETS_ROOT_DIR, 'TEDLIUM_5S')\n",
    "    input_noise_path = os.path.join(DATASETS_ROOT_DIR, \n",
    "                                    'TEDLIUM_noise_sample_5S')\n",
    "\n",
    "    for filename in os.listdir(input_original_path):\n",
    "        \n",
    "        # Link same filename in noise path\n",
    "        filename_component = filename.split('_')\n",
    "        filename_noise = (filename_component[0] +\n",
    "                          '_' +\n",
    "                          filename_component[1] +\n",
    "                          '_' +\n",
    "                          'noise_sample' +\n",
    "                          '_' +\n",
    "                          filename_component[2])\n",
    "        input_original_filename = os.path.join(input_original_path, \n",
    "                                               filename)\n",
    "        input_noise_filename = os.path.join(input_noise_path, filename_noise)\n",
    "        \n",
    "        if not os.path.isfile(input_original_filename):\n",
    "            continue\n",
    "        \n",
    "        original_noise_pairs.append(\n",
    "            [input_original_filename, input_noise_filename])\n",
    "\n",
    "    # Shuffle the datasets\n",
    "    np.random.shuffle(original_noise_pairs)\n",
    "    datasets_size = len(original_noise_pairs)\n",
    "    \n",
    "    # Create indexs\n",
    "    validation_start_index = 0\n",
    "    validation_end_index = (validation_start_index + \n",
    "                            int(datasets_size * validation_fraction))\n",
    "    test_start_index = validation_end_index\n",
    "    test_end_index = (test_start_index + \n",
    "                      int(datasets_size * test_fraction))\n",
    "    train_start_index = test_end_index\n",
    "    \n",
    "    # Save pairs into .csv\n",
    "    validation_original_noise_pairs = original_noise_pairs[\n",
    "        validation_start_index:validation_end_index]\n",
    "    write_csv(os.path.join(OUTPUT_DIR, 'validation_files.csv'),\n",
    "              validation_original_noise_pairs)\n",
    "    test_original_noise_pairs = original_noise_pairs[\n",
    "        test_start_index : test_end_index]\n",
    "    write_csv(os.path.join(OUTPUT_DIR, 'test_files.csv'), \n",
    "              test_original_noise_pairs)\n",
    "    train_original_noise_pairs = original_noise_pairs[\n",
    "        train_start_index :]\n",
    "    write_csv(os.path.join(OUTPUT_DIR, 'train_files.csv'), \n",
    "              original_noise_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
